{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet classification with naive bayes\n",
    "\n",
    "For this notebook we are going to implement a naive bayes classifier for classifying positive or negative based on the words in the tweet. Recall that for two events A and B the bayes theorem says\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "where P(A) and P(B) is the ***class probabilities*** and P(B|A) is called ***conditional probabilities***. this gives us the probability of A happening, given that B has occurred. So as an example if we want to find the probability of \"is this a positive tweet given that it contains the word \"good\" \" we will obtain the following \n",
    "\n",
    "$$ P(\\text{\"positive\"}|\\text{\"good\" in tweet}) = \\frac{P(\"\\text{\"good\" in tweet}|\\text{\"positive\"})P(\\text{\"positive\"})}{P(\"\\text{\"good\" in tweet})} $$\n",
    "\n",
    "This means that to find the probability of \"is this a positive tweet given that it contains the word \"good\" \" we need the probability of \"good\" being in a positive tweet, the probability of a tweet being positive and the probability of \"good\" being in a tweet. \n",
    "\n",
    "Similarly, if we want to obtain the opposite \"is this a negative tweet given that it contains the word \"boring\" \"\n",
    "we get \n",
    "\n",
    "$$ P(\\text{\"negative\"}|\\text{\"boring\" in tweet}) = \\frac{P(\\text{\"boring\" in tweet}|\\text{\"negative\"})P(\\text{\"negative\"})}{P(\\text{\"boring\" in tweet})} $$\n",
    "\n",
    "where we need the probability of \"boring\" being in a negative tweet, the probability of a tweet negative being and the probability of \"boring\" being in a tweet. \n",
    "\n",
    "We can now build a classifier where we compare those two probabilities and whichever is the larger one it's classified as \n",
    "\n",
    "if P(\"positive\"|\"good\" in tweet) $>$ P(\"negative\"|\"boring\" in tweet)\n",
    "    \n",
    "   Tweet is positive\n",
    "\n",
    "else\n",
    "   \n",
    "   Tweet is negative\n",
    "\n",
    "Now let's expand this to handle multiple features and put the Naive assumption into bayes theorem. This means that if features are independent we have \n",
    "\n",
    "$$ P(A,B) = P(A)P(B) $$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{P(b_1|A)P(b_2|A)...P(b_n|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{\\prod_i^nP(b_i|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "\n",
    "So with our previous example expanded with more words \"is this a positive tweet given that it contains the word \"good\" and \"interesting\" \" gives us \n",
    "\n",
    "$$ P(\\text{\"positive\"}|\\text{\"good\", \"interesting\" in tweet}) = \\frac{P(\\text{\"good\" in tweet}|\\text{\"positive\"})P(\\text{\"interesting\" in tweet}|\\text{\"positive\"})P(\\text{\"positive\"})}{P(\\text{\"good\" in tweet})P(\\text{\"interesting\" in tweet})} $$\n",
    "\n",
    "As you can see the denominator remains constant which means we can remove it and the final classifier end up\n",
    "\n",
    "$$y = argmax_A P(A)\\prod_i^nP(b_i|A) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:37:14.983555600Z",
     "start_time": "2024-01-07T13:37:13.641527800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6437/3865003465.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#stuff to import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:59:08.449137900Z",
     "start_time": "2024-01-07T13:59:07.974610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hanging out with my friend waiting for a rain ...</td>\n",
       "      <td>anging friend waiting rain band looking huge s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>yesdir... layin not feelin good</td>\n",
       "      <td>esdir layin feelin good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Hae a nice night</td>\n",
       "      <td>nice night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Tay...where are you? miss you SO bad</td>\n",
       "      <td>aywhere miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>@NeilMcDaid shizer!</td>\n",
       "      <td>neilmcdaid shizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199374</th>\n",
       "      <td>199995</td>\n",
       "      <td>1</td>\n",
       "      <td>Eating cobb salad w my Patricia at CF.</td>\n",
       "      <td>ating cobb salad patricia compare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199375</th>\n",
       "      <td>199996</td>\n",
       "      <td>1</td>\n",
       "      <td>@hallowed_ground ever been to fremont? You cou...</td>\n",
       "      <td>hallowedground ever fremont could help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199376</th>\n",
       "      <td>199997</td>\n",
       "      <td>0</td>\n",
       "      <td>@StacyLynn1985 as far as anything to scrape th...</td>\n",
       "      <td>stacylynn1985 anything scrape teeth clue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199377</th>\n",
       "      <td>199998</td>\n",
       "      <td>1</td>\n",
       "      <td>ksh scripting today to produce html code to di...</td>\n",
       "      <td>scripting today produce html code display pgra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199378</th>\n",
       "      <td>199999</td>\n",
       "      <td>1</td>\n",
       "      <td>@frelle  Ohhh I've love a Chai too!</td>\n",
       "      <td>frelle ohhh love chai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199379 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  sentiment  \\\n",
       "0                0          0   \n",
       "1                1          0   \n",
       "2                2          1   \n",
       "3                3          0   \n",
       "4                4          0   \n",
       "...            ...        ...   \n",
       "199374      199995          1   \n",
       "199375      199996          1   \n",
       "199376      199997          0   \n",
       "199377      199998          1   \n",
       "199378      199999          1   \n",
       "\n",
       "                                                    tweet  \\\n",
       "0       Hanging out with my friend waiting for a rain ...   \n",
       "1                        yesdir... layin not feelin good    \n",
       "2                                       Hae a nice night    \n",
       "3                   Tay...where are you? miss you SO bad    \n",
       "4                                   @NeilMcDaid shizer!     \n",
       "...                                                   ...   \n",
       "199374            Eating cobb salad w my Patricia at CF.    \n",
       "199375  @hallowed_ground ever been to fremont? You cou...   \n",
       "199376  @StacyLynn1985 as far as anything to scrape th...   \n",
       "199377  ksh scripting today to produce html code to di...   \n",
       "199378               @frelle  Ohhh I've love a Chai too!    \n",
       "\n",
       "                                         processed_tweets  \n",
       "0       anging friend waiting rain band looking huge s...  \n",
       "1                                 esdir layin feelin good  \n",
       "2                                              nice night  \n",
       "3                                            aywhere miss  \n",
       "4                                       neilmcdaid shizer  \n",
       "...                                                   ...  \n",
       "199374                  ating cobb salad patricia compare  \n",
       "199375             hallowedground ever fremont could help  \n",
       "199376           stacylynn1985 anything scrape teeth clue  \n",
       "199377  scripting today produce html code display pgra...  \n",
       "199378                              frelle ohhh love chai  \n",
       "\n",
       "[199379 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets=pd.read_csv('data_for_theoretical_notebook_1.csv',encoding='latin')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split the data into a training set and a test set using scikit-learns train_test_split function \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:37:59.025504100Z",
     "start_time": "2024-01-07T13:37:59.020482200Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_data = tweets[\"processed_tweets\"]\n",
    "tweets_labels = tweets[\"sentiment\"]\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweets_data, tweets_labels)\n",
    "\n",
    "#Split data into train_tweets, test_tweets, train_labels and test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to build our classifier is \"probability of positive tweet\" P(pos) , \"probability of negative tweet\" P(neg), \"probability of word in tweet given tweet is positive\" P(w|pos) and \"probability of word in tweet given tweet is negative\" P(w|neg). Start by calculating the probability that a tweet is positive and negative respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets, pos_tweets = tweets_labels.value_counts()\n",
    "tot_tweets = neg_tweets + pos_tweets\n",
    "P_pos = pos_tweets/tot_tweets\n",
    "P_neg = 1 - P_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For P(w|pos), P(w|neg) we need to count how many tweets each word occur in. Count the number of tweets each word occurs in and store in the word counter. An entry in the word counter is for instance {'good': 'Pos':150, 'Neg': 10} meaning good occurs in 150 positive tweets and 10 negative tweets. Be aware that we are not interested in calculating multiple occurrences of the same word in the same tweet. Also, we change the labels from 0 for \"Negative\" and 1 for \"Positive\" to \"Neg\" and \"Pos\" respectively.For each word convert it to lower case. You can use Python's [lower](https://www.w3schools.com/python/ref_string_lower.asp). Another handy Python string method is [split](https://www.w3schools.com/python/ref_string_split.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_labels = train_labels.replace(0, \"Neg\", regex=True)\n",
    "final_train_labels = new_train_labels.replace(1, \"Pos\", regex=True)\n",
    "\n",
    "word_counter = {}\n",
    "\n",
    "for (tweet, label) in zip(train_tweets, final_train_labels):\n",
    "    words = tweet.split()\n",
    "    words = set(words)\n",
    "\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "\n",
    "        if word not in word_counter:\n",
    "            word_counter[word] = {\"Neg\": 0, \"Pos\": 0}\n",
    "\n",
    "        word_counter[word][label] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with a smaller subset of words just to save up some time. Find the 1500 most occuring words in tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_words_to_use = 1500\n",
    "popular_words = sorted(word_counter.items(), key=lambda x: x[1]['Pos'] + x[1]['Neg'], reverse=True)\n",
    "popular_words = [x[0] for x in popular_words[:nr_of_words_to_use]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute P(w|pos), P(w|neg) for the popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_w_given_pos = {}\n",
    "P_w_given_neg = {}\n",
    "\n",
    "for word in popular_words:\n",
    "    n_pos = word_counter[word][\"Pos\"]\n",
    "    n_neg = word_counter[word][\"Neg\"]\n",
    "\n",
    "    P_w_given_pos[word] = n_pos / neg_tweets\n",
    "    P_w_given_neg[word] = n_neg / pos_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = {\n",
    "    'basis'  : popular_words,\n",
    "    'P(pos)'   : P_pos,\n",
    "    'P(neg)'   : P_neg,\n",
    "    'P(w|pos)' : P_w_given_pos,\n",
    "    'P(w|neg)' : P_w_given_neg\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tweet_classifier function that takes your trained classifier and a tweet and returns wether it's about Positive or Negative using the popular words selected. Note that if there are words in the basis words in our classifier that are not in the tweet we have the opposite probabilities i.e P(w_1 occurs )* P(w_2 does not occur) * .... if w_1 occurs and w_2 does not occur. The function should return wether the tweet is Positive or Negative. i.e 'Pos' or 'Neg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_classifier(tweet, classifier_dict):\n",
    "    \"\"\" param tweet: string containing tweet message\n",
    "        param classifier: dict containing 'basis' - training words\n",
    "                                          'P(pos)' - class probabilities\n",
    "                                          'P(neg)' - class probabilities\n",
    "                                          'P(w|pos)' - conditional probabilities\n",
    "                                          'P(w|neg)' - conditional probabilities\n",
    "        \n",
    "        return: either 'Pos' or 'Neg'\n",
    "    \"\"\"\n",
    "    prob_pos = classifier_dict['P(pos)']\n",
    "    prob_neg = classifier_dict['P(neg)']\n",
    "    words = list(map(str.lower, tweet.split()))\n",
    "\n",
    "    for basis_word in classifier_dict[\"basis\"]:\n",
    "        if basis_word in words:\n",
    "            prob_pos *= classifier_dict[\"P(w|pos)\"][basis_word]\n",
    "            prob_neg *= classifier_dict[\"P(w|neg)\"][basis_word]\n",
    "        else:\n",
    "            prob_pos *= 1 - classifier_dict[\"P(w|pos)\"][basis_word]\n",
    "            prob_neg *= 1 - classifier_dict[\"P(w|neg)\"][basis_word]\n",
    "\n",
    "    if prob_pos >= prob_neg:\n",
    "        return \"Pos\"\n",
    "    #else\n",
    "    return \"Neg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier, test_tweets, test_labels):\n",
    "    total = len(test_tweets)\n",
    "    correct = 0\n",
    "    for (tweet,label) in zip(test_tweets, test_labels):\n",
    "        predicted = tweet_classifier(tweet,classifier)\n",
    "        if predicted == label:\n",
    "            correct = correct + 1\n",
    "    return(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_labels = test_labels.replace(0, \"Neg\", regex=True)\n",
    "final_test_labels = new_test_labels.replace(1, \"Pos\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7206\n"
     ]
    }
   ],
   "source": [
    "acc = test_classifier(classifier, test_tweets, final_test_labels)\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional work\n",
    "\n",
    "In basic sentiment analysis classifications we have 3 classes \"Positive\", \"Negative\" and \"Neutral\". Although because it is challenging to create the \"Neutral\" class. Try to improve the accuracy by filtering the dataset from the perspective of removing words that indicate neutrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
